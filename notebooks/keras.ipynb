{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data available at:\n",
    "https://www.kaggle.com/kaushik3497/imdb-sentiment-analysis\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Sequence, Dict, Callable, Any, List, Pattern, Union, Iterable, overload\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer, PunktSentenceTokenizer\n",
    "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger\n",
    "from nltk.corpus import brown\n",
    "import toolz as tz\n",
    "from pattern.en import parse\n",
    "from functools import partial\n",
    "import csv\n",
    "import toolz.curried as tzc\n",
    "import pandas as pd\n",
    "\n",
    "import pattern\n",
    "from pattern.web import plaintext\n",
    "\n",
    "import bs4\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = Path('../data/labeledTrainData.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = pd.read_csv(train_file, sep='\\t', quoting=csv.QUOTE_NONE) \n",
    "# We'll handle quotes manually.\n",
    "\n",
    "raw_train_dataset, raw_test_dataset = train_test_split(raw_dataset, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_curated_english_contractions():\n",
    "    with open('../data/curated_contractions.csv') as f:\n",
    "        reader = csv.reader(f)\n",
    "        contractions = [row for row in reader]\n",
    "    return contractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUOTES_PATTERN = [(r'\\\\\\\"|\\\\\\'', '')]\n",
    "PARENTHESIS_PATTERN = [(r'[]}{)(]', '')]\n",
    "ENGLISH_CONTRACTIONS = get_curated_english_contractions()\n",
    "\n",
    "# We manually curate the stopwords to not remove words that may indicate some form of polarity.\n",
    "STOPWORDS = [\n",
    "    'the', '.', 'a', 'and', ',', 'of', 'to', 'is', 'this', \n",
    "    'it',  'that', 'i', 'but', 'for', 'with', \n",
    "    'was', 'as', 'have', 'on', \"'s\", 'has', 'are',\n",
    "    'be', 'one', 'you', 'at', 'all', 'an', 'from', \n",
    "    'by', 'like', 'so', 'who', 'they', 'his', 'do', \n",
    "    'there', 'about', 'if',  'or', 'he', 'can', 'what',\n",
    "    'when', 'would',  'had',\n",
    "    'time', 'even', 'only', 'will',  'see', 'my', \n",
    "    'which', 'me', 'than', 'did', 'does',\n",
    "    'were', 'their', 'could', 'get', 'been', 'other',\n",
    "    'into', 'her', 'also', 'how', 'because'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_quotes_and_ids(dataset):\n",
    "    no_id_dataset = dataset.drop('id', 'columns')\n",
    "    no_id_dataset['review'] = no_id_dataset['review'].str.strip('\\\"\"')\n",
    "    return no_id_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_quotes(text, pattern: re.Pattern = QUOTES_PATTERN):\n",
    "    '''\n",
    "    removes escaped quotes. (\\' or \\\")\n",
    "    '''\n",
    "    text = pattern.sub('', text)\n",
    "    return text\n",
    "\n",
    "def clean_html(text):\n",
    "    '''\n",
    "    removes html tags.\n",
    "    '''\n",
    "    return plaintext(text)\n",
    "\n",
    "@tz.curry\n",
    "def filter_stopwords(stopwords, tokens):\n",
    "    return tz.filter(lambda t: t not in stopwords, tokens)\n",
    "\n",
    "class RegexpReplacer:\n",
    "    def __init__(self, patterns: Sequence[Tuple[Union[str, re.Pattern], str]]):\n",
    "        self.patterns = [(re.compile(p), r) for p, r in patterns]\n",
    "    \n",
    "    def replace(self, text: str) -> str:\n",
    "        for pattern, repl in self.patterns:\n",
    "            text = pattern.sub(repl, text)\n",
    "        return text\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        return self.replace(text)\n",
    "    \n",
    "\n",
    "def process_text(text, func):\n",
    "    '''\n",
    "    clean and tokenize a text.\n",
    "    tokenize: bool indicate if the function should tokenize.\n",
    "    resolve: bool indicates if the iterator should be outputted to a list or not.\n",
    "    '''\n",
    "    return func(text)\n",
    "    \n",
    "def create_corpus_processor(*steps):\n",
    "    '''\n",
    "    Produces a function that can be applied to a corpus of document, applying each step in series to each document.\n",
    "    '''\n",
    "                   \n",
    "    def process_corpus(corpus: Iterable[str], \n",
    "                       map: Callable[[Callable, Iterable], Iterable] = tz.map,\n",
    "                       collect=None) -> Iterable[str]:\n",
    "        '''\n",
    "        Process a corpus, represented as an iterable of text into a clean and tokenized corpus.\n",
    "        Downstream tasks can be mapped to the return iterable.\n",
    "        You can provide a custom map, for example to process the items in parallel.\n",
    "        '''\n",
    "        \n",
    "        func = tz.compose(collect or tz.identity, *reversed(steps)) # compose applies last step first.\n",
    "        apply_steps = partial(process_text, func=func)\n",
    "        processed_corpus = tz.map(apply_steps, corpus)\n",
    "        return processed_corpus\n",
    "    \n",
    "    return process_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_remover = RegexpReplacer(QUOTES_PATTERN)\n",
    "parenthesis_remover = RegexpReplacer(PARENTHESIS_PATTERN)\n",
    "contraction_replacer = RegexpReplacer(ENGLISH_CONTRACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer = PunktSentenceTokenizer()\n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "tokenize_text = tzc.compose(tzc.mapcat(word_tokenizer.tokenize), sent_tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_corpus = create_corpus_processor(quote_remover, \n",
    "                                        parenthesis_remover, \n",
    "                                        contraction_replacer, \n",
    "                                        clean_html,\n",
    "                                        str.lower, \n",
    "                                        tokenize_text)\n",
    "                                    \n",
    "                        \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>0</td>\n",
       "      <td>Some people seem to think this was the worst m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12674</th>\n",
       "      <td>0</td>\n",
       "      <td>I registered just to make this comment (which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24507</th>\n",
       "      <td>1</td>\n",
       "      <td>The best Cheech &amp; Chong movie so far!! Of all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13940</th>\n",
       "      <td>1</td>\n",
       "      <td>Return To The 3th Chamber is the comedic seque...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>1</td>\n",
       "      <td>I wish \\\"that '70s show\\\" would come back on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1</td>\n",
       "      <td>This was the first PPV in a new era for the WW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17024</th>\n",
       "      <td>1</td>\n",
       "      <td>The Sopranos (now preparing to end) is the ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6744</th>\n",
       "      <td>0</td>\n",
       "      <td>... You can't exactly shove her out of the way...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5802</th>\n",
       "      <td>0</td>\n",
       "      <td>Yep, the topic is a straight quote from the mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18440</th>\n",
       "      <td>1</td>\n",
       "      <td>I will start by saying that this has undeserve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                             review\n",
       "1383           0  Some people seem to think this was the worst m...\n",
       "12674          0  I registered just to make this comment (which ...\n",
       "24507          1  The best Cheech & Chong movie so far!! Of all ...\n",
       "13940          1  Return To The 3th Chamber is the comedic seque...\n",
       "6088           1  I wish \\\"that '70s show\\\" would come back on t...\n",
       "182            1  This was the first PPV in a new era for the WW...\n",
       "17024          1  The Sopranos (now preparing to end) is the ver...\n",
       "6744           0  ... You can't exactly shove her out of the way...\n",
       "5802           0  Yep, the topic is a straight quote from the mo...\n",
       "18440          1  I will start by saying that this has undeserve..."
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = strip_quotes_and_ids(raw_train_dataset)\n",
    "y = dataset['sentiment']\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProcessPoolExecutor() as executor:\n",
    "    map_func = partial(executor.map, chunksize=1000)\n",
    "    processed_corpus = list(process_corpus(dataset['review'], map_func, list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train, corpus_devtest, y_train, y_devtest = train_test_split(processed_corpus, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2735"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max(tz.map(len, corpus_train))\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_features, oov_token='<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1111, 1539, 1, 9443, 24, 1288, 6, 2951, 116, 203, 450, 204, 16, 1141, 81, 29, 24, 1, 1466, 1, 4461, 3353, 18, 2, 6439, 7, 170, 7, 2, 245, 203, 23, 131, 102, 4, 9443, 9, 354, 2, 1693, 1905, 7, 2, 166, 9444, 4, 13, 23, 9, 17, 5319, 5, 1247, 17, 2, 1, 7, 1, 4, 12, 110, 202, 2460, 5, 2811, 8, 142, 14, 12, 290, 10, 11, 1, 3, 1, 4590, 8, 75, 46, 475, 2, 1468, 87, 31, 278, 4, 2, 1480, 9, 81, 1, 118, 9443, 1, 6, 4695, 5, 3464, 10, 4, 2, 248, 750, 19, 2, 182, 7, 1933, 1, 4, 70, 269, 39, 24, 1483, 11, 3624, 4495, 3, 123, 179, 7937, 9603, 3, 903, 5, 1, 9, 38, 275, 45, 3159, 1018, 8, 31, 1918, 3, 87, 686, 368, 98, 9443, 1288, 8, 176, 41, 116, 4038, 159, 1650, 8, 1, 30, 629, 789, 19, 6, 778, 400, 4, 2, 277, 113, 3, 6, 7347, 1545, 967, 3, 9, 255, 37, 6, 1111, 275, 19, 6, 3966, 1111, 1149, 4, 1150, 40, 120, 6, 1430, 436, 3, 29, 9, 92, 2, 6160, 1545, 967, 11, 1103, 4, 451, 2, 194, 14, 29, 2880, 1310, 352, 9870, 5, 301, 104, 90, 9031, 3, 101, 3247, 104, 90, 30, 348, 69, 5918, 9, 2, 418, 1780, 7, 6, 6722, 8058, 3, 5, 79, 35, 13, 11, 992, 1679, 7, 30, 5681, 6215, 3, 2, 1, 2923, 8, 3680, 3, 164, 41, 236, 17, 8, 3181, 1262, 32, 7, 30, 1, 11, 6, 5098, 433, 8721, 27, 30, 1653, 4, 2, 717, 26, 7086, 93, 4, 2, 366, 5450, 22, 413, 647, 1897, 46, 195, 271, 18, 6, 54, 7, 2480, 118, 195, 8, 2480, 1, 1, 27, 6, 3547, 1647, 4, 235, 2, 159, 74, 520, 1, 1, 145, 6, 601, 9050, 7, 1862, 1, 842, 1, 49, 36, 1013, 11, 73, 2161, 5120, 3, 1, 3, 1, 3, 5, 1639, 4748, 352, 145, 5, 235, 124, 49, 106, 271, 16, 1054, 27, 717, 3, 307, 1, 3, 1916, 45, 91, 366, 1088, 3, 10, 9, 70, 1194, 8, 715, 14, 36, 78, 3747, 18, 6, 175, 1, 551, 7010, 4, 1008, 3, 36, 87, 15, 4268, 6, 551, 505, 118, 2, 1, 1821, 3643, 2, 1850, 11, 6, 9817, 166, 1, 2392, 4, 56, 82, 359, 7, 2, 72, 2508, 27, 2, 1551, 7899, 7, 257, 5471, 1111, 1, 1838, 5, 2, 1, 1983, 7, 119, 190, 1, 8, 617, 99, 4, 32, 7, 140, 190, 159, 9, 261, 1, 94, 29, 24, 2, 70, 275, 11, 13, 221, 906, 19, 64, 6, 3302, 322, 34, 6, 155, 600, 11, 2, 108, 4, 2, 91, 2196, 209, 11, 3, 19, 6, 1, 1, 5, 3845, 2881, 7025, 14, 701, 8, 31, 117, 8, 31, 2602, 4, 2, 1540, 177, 7, 2, 23, 9, 123, 2, 421, 217, 100, 828, 3, 17, 2, 1519, 7, 257, 436, 26, 1, 109, 29, 1457, 6, 242, 39, 60, 46, 5510, 53, 6793, 1, 8, 28, 6, 8299, 3662, 268, 27, 44, 4, 47, 373, 928, 146, 6, 242, 40, 9740, 61, 4691, 8, 1, 11, 5562, 7, 120, 1825, 3, 46, 1743, 9443, 22, 2042, 5, 342, 2, 1803, 1264, 4, 109, 84, 9198, 1, 50, 7, 1, 1, 9512, 3, 2, 828, 512, 1895, 8, 105, 73, 1212, 4, 36, 174, 582, 17, 36, 78, 165, 337, 3, 276, 64, 1, 3, 19, 328, 1, 2365, 1, 1, 192, 73, 502, 4, 291, 169, 42, 1067, 3, 36, 174, 56, 42, 327, 1572, 4436, 1622, 18, 2, 1, 4, 34, 13, 216, 11, 2, 20, 9443, 8414, 6, 1489, 868, 40, 30, 1, 1, 3353, 3, 5, 1, 1020, 8672, 7, 931, 1, 1, 90, 2, 777, 4, 76, 1154, 645, 9793, 8098, 1, 3538, 4, 522, 36, 232, 28, 255, 11, 38, 1, 7678, 57, 9443, 16, 137, 190, 195, 8, 4895, 30, 1, 4, 964, 91, 1067, 3, 9443, 22, 512, 1095, 5, 5697, 17, 1239, 68, 8157, 3, 12, 375, 3, 17, 36, 78, 165, 1494, 1, 4, 13, 394, 257, 8408, 7405, 4699, 8, 2927, 171, 19, 56, 7, 30, 1953, 789, 4, 412, 3, 6, 1953, 2927, 10, 9, 4, 12, 62, 15, 2433, 230, 37, 3796, 2, 1511, 279, 4, 35, 12, 54, 142, 9, 10, 24, 905, 11, 3209, 19, 2, 344, 7, 13, 957, 4, 2, 1073, 7, 1693, 1905, 443, 27, 68, 45, 141, 12, 142, 30, 1, 4]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(corpus_train[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(corpus, max_length):\n",
    "    indexed_corpus = tokenizer.texts_to_sequences(corpus)\n",
    "    return pad_sequences(indexed_corpus, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = transform(corpus_train, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(vocab_size, dim, input_length):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, dim, input_length=input_length))\n",
    "    model.add(layers.Conv1D(16, 5, activation='relu'))\n",
    "    model.add(layers.MaxPool1D(5))\n",
    "#     model.add(layers.Flatten())\n",
    "#     model.add(layers.Dropout(0.33))\n",
    "    model.add(layers.Conv1D(16, 5, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPool1D())\n",
    "    model.add(layers.Dropout(0.33))\n",
    "    model.add(layers.Dense(8, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc']\n",
    "                 )\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 2735, 64)          640000    \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 2731, 16)          5136      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 546, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 542, 16)           1296      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 646,577\n",
      "Trainable params: 646,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build(tokenizer.num_words, 64, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14400 samples, validate on 3600 samples\n",
      "Epoch 1/20\n",
      "14400/14400 [==============================] - 27s 2ms/sample - loss: 0.6433 - acc: 0.6092 - val_loss: 0.4265 - val_acc: 0.8286\n",
      "Epoch 2/20\n",
      "14400/14400 [==============================] - 27s 2ms/sample - loss: 0.3683 - acc: 0.8447 - val_loss: 0.3182 - val_acc: 0.8669\n",
      "Epoch 3/20\n",
      "14400/14400 [==============================] - 27s 2ms/sample - loss: 0.2288 - acc: 0.9165 - val_loss: 0.3302 - val_acc: 0.8692\n",
      "Epoch 4/20\n",
      "14400/14400 [==============================] - 27s 2ms/sample - loss: 0.1502 - acc: 0.9492 - val_loss: 0.3392 - val_acc: 0.8678\n",
      "Epoch 5/20\n",
      "14400/14400 [==============================] - 28s 2ms/sample - loss: 0.0999 - acc: 0.9697 - val_loss: 0.3994 - val_acc: 0.8639\n",
      "Epoch 6/20\n",
      "14400/14400 [==============================] - 27s 2ms/sample - loss: 0.0663 - acc: 0.9799 - val_loss: 0.4697 - val_acc: 0.8614\n",
      "Epoch 7/20\n",
      "14400/14400 [==============================] - 26s 2ms/sample - loss: 0.0462 - acc: 0.9856 - val_loss: 0.5222 - val_acc: 0.8622\n",
      "Epoch 8/20\n",
      "14400/14400 [==============================] - 27s 2ms/sample - loss: 0.0371 - acc: 0.9888 - val_loss: 0.5696 - val_acc: 0.8608\n",
      "Epoch 9/20\n",
      "14400/14400 [==============================] - 27s 2ms/sample - loss: 0.0296 - acc: 0.9908 - val_loss: 0.6182 - val_acc: 0.8561\n",
      "Epoch 10/20\n",
      "14400/14400 [==============================] - 27s 2ms/sample - loss: 0.0306 - acc: 0.9909 - val_loss: 0.6623 - val_acc: 0.8486\n",
      "Epoch 11/20\n",
      "14400/14400 [==============================] - 27s 2ms/sample - loss: 0.0248 - acc: 0.9916 - val_loss: 0.7037 - val_acc: 0.8550\n",
      "Epoch 12/20\n",
      "14400/14400 [==============================] - 27s 2ms/sample - loss: 0.0269 - acc: 0.9915 - val_loss: 0.7089 - val_acc: 0.8608\n",
      "Epoch 13/20\n",
      "14400/14400 [==============================] - 26s 2ms/sample - loss: 0.0298 - acc: 0.9902 - val_loss: 0.7022 - val_acc: 0.8578\n",
      "Epoch 14/20\n",
      "14400/14400 [==============================] - 1036s 72ms/sample - loss: 0.0179 - acc: 0.9941 - val_loss: 0.7307 - val_acc: 0.8611\n",
      "Epoch 15/20\n",
      "14400/14400 [==============================] - 27s 2ms/sample - loss: 0.0168 - acc: 0.9939 - val_loss: 0.8113 - val_acc: 0.8486\n",
      "Epoch 16/20\n",
      "14400/14400 [==============================] - 28s 2ms/sample - loss: 0.0206 - acc: 0.9925 - val_loss: 0.7590 - val_acc: 0.8533\n",
      "Epoch 17/20\n",
      "14400/14400 [==============================] - 28s 2ms/sample - loss: 0.0158 - acc: 0.9944 - val_loss: 0.7873 - val_acc: 0.8558\n",
      "Epoch 18/20\n",
      "14400/14400 [==============================] - 27s 2ms/sample - loss: 0.0200 - acc: 0.9931 - val_loss: 0.8091 - val_acc: 0.8572\n",
      "Epoch 19/20\n",
      "14400/14400 [==============================] - 27s 2ms/sample - loss: 0.0173 - acc: 0.9935 - val_loss: 0.9057 - val_acc: 0.8508\n",
      "Epoch 20/20\n",
      "14400/14400 [==============================] - 27s 2ms/sample - loss: 0.0162 - acc: 0.9941 - val_loss: 0.8400 - val_acc: 0.8592\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=20, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_devtest = transform(corpus_devtest, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict_classes(x_devtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.85      0.86      2254\n",
      "         pos       0.85      0.87      0.86      2246\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      4500\n",
      "   macro avg       0.86      0.86      0.86      4500\n",
      "weighted avg       0.86      0.86      0.86      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_devtest, y_hat, target_names=['neg', 'pos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
