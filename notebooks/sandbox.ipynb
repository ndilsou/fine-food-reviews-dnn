{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer, PunktSentenceTokenizer\n",
    "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger\n",
    "from nltk.corpus import brown\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "import csv\n",
    "import spacy\n",
    "import toolz as tz\n",
    "import toolz.curried as tzc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledTrainData.tsv   testData.tsv\n",
      "sampleSubmission.csv   unlabeledTrainData.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = Path('../data/labeledTrainData.tsv')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = train_file.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = csv.DictReader(f, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tz.take(10, records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = tuple(tz.pluck('review', samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\The Classic War of the Worlds\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\"critics\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\"critics\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\"critics\\\" perceive to be its shortcomings.\"\n"
     ]
    }
   ],
   "source": [
    "review = reviews[1]\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\\\The Classic War of the Worlds\\\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells\\' classic book.',\n",
       " 'Mr. Hines succeeds in doing so.',\n",
       " 'I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g.',\n",
       " 'the Spielberg version with Tom Cruise that had only the slightest resemblance to the book.',\n",
       " 'Obviously, everyone looks for different things in a movie.',\n",
       " 'Those who envision themselves as amateur \\\\\"critics\\\\\" look only to criticize everything they can.',\n",
       " 'Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\\\"critics\\\\\".',\n",
       " 'We enjoyed the effort Mr. Hines put into being faithful to H.G.',\n",
       " \"Wells' classic novel, and we found it to be very entertaining.\",\n",
       " 'This made it easy to overlook what the \\\\\"critics\\\\\" perceive to be its shortcomings.\"']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = nltk.sent_tokenize(review)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_words = nltk.word_tokenize(review)\n",
    "gold_words;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_regex = re.compile(r'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module re:\n",
      "\n",
      "NAME\n",
      "    re - Support for regular expressions (RE).\n",
      "\n",
      "MODULE REFERENCE\n",
      "    https://docs.python.org/3.7/library/re\n",
      "    \n",
      "    The following documentation is automatically generated from the Python\n",
      "    source files.  It may be incomplete, incorrect or include features that\n",
      "    are considered implementation detail and may vary between Python\n",
      "    implementations.  When in doubt, consult the module reference at the\n",
      "    location listed above.\n",
      "\n",
      "DESCRIPTION\n",
      "    This module provides regular expression matching operations similar to\n",
      "    those found in Perl.  It supports both 8-bit and Unicode strings; both\n",
      "    the pattern and the strings being processed can contain null bytes and\n",
      "    characters outside the US ASCII range.\n",
      "    \n",
      "    Regular expressions can contain both special and ordinary characters.\n",
      "    Most ordinary characters, like \"A\", \"a\", or \"0\", are the simplest\n",
      "    regular expressions; they simply match themselves.  You can\n",
      "    concatenate ordinary characters, so last matches the string 'last'.\n",
      "    \n",
      "    The special characters are:\n",
      "        \".\"      Matches any character except a newline.\n",
      "        \"^\"      Matches the start of the string.\n",
      "        \"$\"      Matches the end of the string or just before the newline at\n",
      "                 the end of the string.\n",
      "        \"*\"      Matches 0 or more (greedy) repetitions of the preceding RE.\n",
      "                 Greedy means that it will match as many repetitions as possible.\n",
      "        \"+\"      Matches 1 or more (greedy) repetitions of the preceding RE.\n",
      "        \"?\"      Matches 0 or 1 (greedy) of the preceding RE.\n",
      "        *?,+?,?? Non-greedy versions of the previous three special characters.\n",
      "        {m,n}    Matches from m to n repetitions of the preceding RE.\n",
      "        {m,n}?   Non-greedy version of the above.\n",
      "        \"\\\\\"     Either escapes special characters or signals a special sequence.\n",
      "        []       Indicates a set of characters.\n",
      "                 A \"^\" as the first character indicates a complementing set.\n",
      "        \"|\"      A|B, creates an RE that will match either A or B.\n",
      "        (...)    Matches the RE inside the parentheses.\n",
      "                 The contents can be retrieved or matched later in the string.\n",
      "        (?aiLmsux) Set the A, I, L, M, S, U, or X flag for the RE (see below).\n",
      "        (?:...)  Non-grouping version of regular parentheses.\n",
      "        (?P<name>...) The substring matched by the group is accessible by name.\n",
      "        (?P=name)     Matches the text matched earlier by the group named name.\n",
      "        (?#...)  A comment; ignored.\n",
      "        (?=...)  Matches if ... matches next, but doesn't consume the string.\n",
      "        (?!...)  Matches if ... doesn't match next.\n",
      "        (?<=...) Matches if preceded by ... (must be fixed length).\n",
      "        (?<!...) Matches if not preceded by ... (must be fixed length).\n",
      "        (?(id/name)yes|no) Matches yes pattern if the group with id/name matched,\n",
      "                           the (optional) no pattern otherwise.\n",
      "    \n",
      "    The special sequences consist of \"\\\\\" and a character from the list\n",
      "    below.  If the ordinary character is not on the list, then the\n",
      "    resulting RE will match the second character.\n",
      "        \\number  Matches the contents of the group of the same number.\n",
      "        \\A       Matches only at the start of the string.\n",
      "        \\Z       Matches only at the end of the string.\n",
      "        \\b       Matches the empty string, but only at the start or end of a word.\n",
      "        \\B       Matches the empty string, but not at the start or end of a word.\n",
      "        \\d       Matches any decimal digit; equivalent to the set [0-9] in\n",
      "                 bytes patterns or string patterns with the ASCII flag.\n",
      "                 In string patterns without the ASCII flag, it will match the whole\n",
      "                 range of Unicode digits.\n",
      "        \\D       Matches any non-digit character; equivalent to [^\\d].\n",
      "        \\s       Matches any whitespace character; equivalent to [ \\t\\n\\r\\f\\v] in\n",
      "                 bytes patterns or string patterns with the ASCII flag.\n",
      "                 In string patterns without the ASCII flag, it will match the whole\n",
      "                 range of Unicode whitespace characters.\n",
      "        \\S       Matches any non-whitespace character; equivalent to [^\\s].\n",
      "        \\w       Matches any alphanumeric character; equivalent to [a-zA-Z0-9_]\n",
      "                 in bytes patterns or string patterns with the ASCII flag.\n",
      "                 In string patterns without the ASCII flag, it will match the\n",
      "                 range of Unicode alphanumeric characters (letters plus digits\n",
      "                 plus underscore).\n",
      "                 With LOCALE, it will match the set [0-9_] plus characters defined\n",
      "                 as letters for the current locale.\n",
      "        \\W       Matches the complement of \\w.\n",
      "        \\\\       Matches a literal backslash.\n",
      "    \n",
      "    This module exports the following functions:\n",
      "        match     Match a regular expression pattern to the beginning of a string.\n",
      "        fullmatch Match a regular expression pattern to all of a string.\n",
      "        search    Search a string for the presence of a pattern.\n",
      "        sub       Substitute occurrences of a pattern found in a string.\n",
      "        subn      Same as sub, but also return the number of substitutions made.\n",
      "        split     Split a string by the occurrences of a pattern.\n",
      "        findall   Find all occurrences of a pattern in a string.\n",
      "        finditer  Return an iterator yielding a Match object for each match.\n",
      "        compile   Compile a pattern into a Pattern object.\n",
      "        purge     Clear the regular expression cache.\n",
      "        escape    Backslash all non-alphanumerics in a string.\n",
      "    \n",
      "    Some of the functions in this module takes flags as optional parameters:\n",
      "        A  ASCII       For string patterns, make \\w, \\W, \\b, \\B, \\d, \\D\n",
      "                       match the corresponding ASCII character categories\n",
      "                       (rather than the whole Unicode categories, which is the\n",
      "                       default).\n",
      "                       For bytes patterns, this flag is the only available\n",
      "                       behaviour and needn't be specified.\n",
      "        I  IGNORECASE  Perform case-insensitive matching.\n",
      "        L  LOCALE      Make \\w, \\W, \\b, \\B, dependent on the current locale.\n",
      "        M  MULTILINE   \"^\" matches the beginning of lines (after a newline)\n",
      "                       as well as the string.\n",
      "                       \"$\" matches the end of lines (before a newline) as well\n",
      "                       as the end of the string.\n",
      "        S  DOTALL      \".\" matches any character at all, including the newline.\n",
      "        X  VERBOSE     Ignore whitespace and comments for nicer looking RE's.\n",
      "        U  UNICODE     For compatibility only. Ignored for string patterns (it\n",
      "                       is the default), and forbidden for bytes patterns.\n",
      "    \n",
      "    This module also defines an exception 'error'.\n",
      "\n",
      "CLASSES\n",
      "    builtins.Exception(builtins.BaseException)\n",
      "        error\n",
      "    builtins.object\n",
      "        Match\n",
      "        Pattern\n",
      "    \n",
      "    class Match(builtins.object)\n",
      "     |  The result of re.match() and re.search().\n",
      "     |  Match objects always have a boolean value of True.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __copy__(self, /)\n",
      "     |  \n",
      "     |  __deepcopy__(self, memo, /)\n",
      "     |  \n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  end(self, group=0, /)\n",
      "     |      Return index of the end of the substring matched by group.\n",
      "     |  \n",
      "     |  expand(self, /, template)\n",
      "     |      Return the string obtained by doing backslash substitution on the string template, as done by the sub() method.\n",
      "     |  \n",
      "     |  group(...)\n",
      "     |      group([group1, ...]) -> str or tuple.\n",
      "     |      Return subgroup(s) of the match by indices or names.\n",
      "     |      For 0 returns the entire match.\n",
      "     |  \n",
      "     |  groupdict(self, /, default=None)\n",
      "     |      Return a dictionary containing all the named subgroups of the match, keyed by the subgroup name.\n",
      "     |      \n",
      "     |      default\n",
      "     |        Is used for groups that did not participate in the match.\n",
      "     |  \n",
      "     |  groups(self, /, default=None)\n",
      "     |      Return a tuple containing all the subgroups of the match, from 1.\n",
      "     |      \n",
      "     |      default\n",
      "     |        Is used for groups that did not participate in the match.\n",
      "     |  \n",
      "     |  span(self, group=0, /)\n",
      "     |      For match object m, return the 2-tuple (m.start(group), m.end(group)).\n",
      "     |  \n",
      "     |  start(self, group=0, /)\n",
      "     |      Return index of the start of the substring matched by group.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  endpos\n",
      "     |      The index into the string beyond which the RE engine will not go.\n",
      "     |  \n",
      "     |  lastgroup\n",
      "     |      The name of the last matched capturing group.\n",
      "     |  \n",
      "     |  lastindex\n",
      "     |      The integer index of the last matched capturing group.\n",
      "     |  \n",
      "     |  pos\n",
      "     |      The index into the string at which the RE engine started looking for a match.\n",
      "     |  \n",
      "     |  re\n",
      "     |      The regular expression object.\n",
      "     |  \n",
      "     |  regs\n",
      "     |  \n",
      "     |  string\n",
      "     |      The string passed to match() or search().\n",
      "    \n",
      "    class Pattern(builtins.object)\n",
      "     |  Compiled regular expression object.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __copy__(self, /)\n",
      "     |  \n",
      "     |  __deepcopy__(self, memo, /)\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __hash__(self, /)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  findall(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |      Return a list of all non-overlapping matches of pattern in string.\n",
      "     |  \n",
      "     |  finditer(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |      Return an iterator over all non-overlapping matches for the RE pattern in string.\n",
      "     |      \n",
      "     |      For each match, the iterator returns a match object.\n",
      "     |  \n",
      "     |  fullmatch(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |      Matches against all of the string.\n",
      "     |  \n",
      "     |  match(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |      Matches zero or more characters at the beginning of the string.\n",
      "     |  \n",
      "     |  scanner(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |  \n",
      "     |  search(self, /, string, pos=0, endpos=9223372036854775807)\n",
      "     |      Scan through string looking for a match, and return a corresponding match object instance.\n",
      "     |      \n",
      "     |      Return None if no position in the string matches.\n",
      "     |  \n",
      "     |  split(self, /, string, maxsplit=0)\n",
      "     |      Split string by the occurrences of pattern.\n",
      "     |  \n",
      "     |  sub(self, /, repl, string, count=0)\n",
      "     |      Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement repl.\n",
      "     |  \n",
      "     |  subn(self, /, repl, string, count=0)\n",
      "     |      Return the tuple (new_string, number_of_subs_made) found by replacing the leftmost non-overlapping occurrences of pattern with the replacement repl.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  flags\n",
      "     |      The regex matching flags.\n",
      "     |  \n",
      "     |  groupindex\n",
      "     |      A dictionary mapping group names to group numbers.\n",
      "     |  \n",
      "     |  groups\n",
      "     |      The number of capturing groups in the pattern.\n",
      "     |  \n",
      "     |  pattern\n",
      "     |      The pattern string from which the RE object was compiled.\n",
      "    \n",
      "    class error(builtins.Exception)\n",
      "     |  error(msg, pattern=None, pos=None)\n",
      "     |  \n",
      "     |  Exception raised for invalid regular expressions.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |  \n",
      "     |      msg: The unformatted error message\n",
      "     |      pattern: The regular expression pattern\n",
      "     |      pos: The index in the pattern where compilation failed (may be None)\n",
      "     |      lineno: The line corresponding to pos (may be None)\n",
      "     |      colno: The column corresponding to pos (may be None)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      error\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, msg, pattern=None, pos=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "\n",
      "FUNCTIONS\n",
      "    compile(pattern, flags=0)\n",
      "        Compile a regular expression pattern, returning a Pattern object.\n",
      "    \n",
      "    escape(pattern)\n",
      "        Escape special characters in a string.\n",
      "    \n",
      "    findall(pattern, string, flags=0)\n",
      "        Return a list of all non-overlapping matches in the string.\n",
      "        \n",
      "        If one or more capturing groups are present in the pattern, return\n",
      "        a list of groups; this will be a list of tuples if the pattern\n",
      "        has more than one group.\n",
      "        \n",
      "        Empty matches are included in the result.\n",
      "    \n",
      "    finditer(pattern, string, flags=0)\n",
      "        Return an iterator over all non-overlapping matches in the\n",
      "        string.  For each match, the iterator returns a Match object.\n",
      "        \n",
      "        Empty matches are included in the result.\n",
      "    \n",
      "    fullmatch(pattern, string, flags=0)\n",
      "        Try to apply the pattern to all of the string, returning\n",
      "        a Match object, or None if no match was found.\n",
      "    \n",
      "    match(pattern, string, flags=0)\n",
      "        Try to apply the pattern at the start of the string, returning\n",
      "        a Match object, or None if no match was found.\n",
      "    \n",
      "    purge()\n",
      "        Clear the regular expression caches\n",
      "    \n",
      "    search(pattern, string, flags=0)\n",
      "        Scan through string looking for a match to the pattern, returning\n",
      "        a Match object, or None if no match was found.\n",
      "    \n",
      "    split(pattern, string, maxsplit=0, flags=0)\n",
      "        Split the source string by the occurrences of the pattern,\n",
      "        returning a list containing the resulting substrings.  If\n",
      "        capturing parentheses are used in pattern, then the text of all\n",
      "        groups in the pattern are also returned as part of the resulting\n",
      "        list.  If maxsplit is nonzero, at most maxsplit splits occur,\n",
      "        and the remainder of the string is returned as the final element\n",
      "        of the list.\n",
      "    \n",
      "    sub(pattern, repl, string, count=0, flags=0)\n",
      "        Return the string obtained by replacing the leftmost\n",
      "        non-overlapping occurrences of the pattern in string by the\n",
      "        replacement repl.  repl can be either a string or a callable;\n",
      "        if a string, backslash escapes in it are processed.  If it is\n",
      "        a callable, it's passed the Match object and must return\n",
      "        a replacement string to be used.\n",
      "    \n",
      "    subn(pattern, repl, string, count=0, flags=0)\n",
      "        Return a 2-tuple containing (new_string, number).\n",
      "        new_string is the string obtained by replacing the leftmost\n",
      "        non-overlapping occurrences of the pattern in the source\n",
      "        string by the replacement repl.  number is the number of\n",
      "        substitutions that were made. repl can be either a string or a\n",
      "        callable; if a string, backslash escapes in it are processed.\n",
      "        If it is a callable, it's passed the Match object and must\n",
      "        return a replacement string to be used.\n",
      "    \n",
      "    template(pattern, flags=0)\n",
      "        Compile a template pattern, returning a Pattern object\n",
      "\n",
      "DATA\n",
      "    A = <RegexFlag.ASCII: 256>\n",
      "    ASCII = <RegexFlag.ASCII: 256>\n",
      "    DOTALL = <RegexFlag.DOTALL: 16>\n",
      "    I = <RegexFlag.IGNORECASE: 2>\n",
      "    IGNORECASE = <RegexFlag.IGNORECASE: 2>\n",
      "    L = <RegexFlag.LOCALE: 4>\n",
      "    LOCALE = <RegexFlag.LOCALE: 4>\n",
      "    M = <RegexFlag.MULTILINE: 8>\n",
      "    MULTILINE = <RegexFlag.MULTILINE: 8>\n",
      "    S = <RegexFlag.DOTALL: 16>\n",
      "    U = <RegexFlag.UNICODE: 32>\n",
      "    UNICODE = <RegexFlag.UNICODE: 32>\n",
      "    VERBOSE = <RegexFlag.VERBOSE: 64>\n",
      "    X = <RegexFlag.VERBOSE: 64>\n",
      "    __all__ = ['match', 'fullmatch', 'search', 'sub', 'subn', 'split', 'fi...\n",
      "\n",
      "VERSION\n",
      "    2.2.1\n",
      "\n",
      "FILE\n",
      "    /Users/ndiladjimsoungadoy/.local/share/virtualenvs/imdb_dataset-ZDLglFKy/lib/python3.7/re.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.regexp_tokenize(review, \n",
    "r'''(?x)\n",
    "\\w+(?:[-'.]\\w+)* | [A-Z]+(?:\\.[A-Z]+)*\n",
    "''')\n",
    "words;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate: 0.05789473684210526\n",
      "{'G.', '\\\\', ',', \"'\", 'critics\\\\', 'H.', 'Mr.', '\\\\The', 'Worlds\\\\', \"''\", '.'}\n"
     ]
    }
   ],
   "source": [
    "diff = set(gold_words).difference(words)\n",
    "error = len(diff) / len(gold_words)\n",
    "print(f'error rate: {error}')\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mTreebankWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.\n",
       "This is the method that is invoked by ``word_tokenize()``.  It assumes that the\n",
       "text has already been segmented into sentences, e.g. using ``sent_tokenize()``.\n",
       "\n",
       "This tokenizer performs the following steps:\n",
       "\n",
       "- split standard contractions, e.g. ``don't`` -> ``do n't`` and ``they'll`` -> ``they 'll``\n",
       "- treat most punctuation characters as separate tokens\n",
       "- split off commas and single quotes, when followed by whitespace\n",
       "- separate periods that appear at the end of line\n",
       "\n",
       "    >>> from nltk.tokenize import TreebankWordTokenizer\n",
       "    >>> s = '''Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\nThanks.'''\n",
       "    >>> TreebankWordTokenizer().tokenize(s)\n",
       "    ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks', '.']\n",
       "    >>> s = \"They'll save and invest more.\"\n",
       "    >>> TreebankWordTokenizer().tokenize(s)\n",
       "    ['They', \"'ll\", 'save', 'and', 'invest', 'more', '.']\n",
       "    >>> s = \"hi, my name can't hello,\"\n",
       "    >>> TreebankWordTokenizer().tokenize(s)\n",
       "    ['hi', ',', 'my', 'name', 'ca', \"n't\", 'hello', ',']\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.local/share/virtualenvs/imdb_dataset-ZDLglFKy/lib/python3.7/site-packages/nltk/tokenize/treebank.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TreebankWordTokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrain_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlang_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtoken_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'nltk.tokenize.punkt.PunktToken'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "A sentence tokenizer which uses an unsupervised algorithm to build\n",
       "a model for abbreviation words, collocations, and words that start\n",
       "sentences; and then uses that model to find sentence boundaries.\n",
       "This approach has been shown to work well for many European\n",
       "languages.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "train_text can either be the sole training text for this sentence\n",
       "boundary detector, or can be a PunktParameters object.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.local/share/virtualenvs/imdb_dataset-ZDLglFKy/lib/python3.7/site-packages/nltk/tokenize/punkt.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PunktSentenceTokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_tokenizer = PunktSentenceTokenizer()\n",
    "words_tokenizer = TreebankWordTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_words = tzc.compose(tzc.mapcat(words_tokenizer.tokenize), sents_tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(tokenize_words(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2997\n"
     ]
    }
   ],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='editorial', tagset='universal')\n",
    "length = len(tagged_sents)\n",
    "print(length)\n",
    "p = 0.8\n",
    "train = tagged_sents[: int(length * p)]\n",
    "test = tagged_sents[int(length * p):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tagger, test_sents):\n",
    "    print(tagger.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.8180710969018802\n",
      "0.8226383496993225\n",
      "0.8224099870594505\n"
     ]
    }
   ],
   "source": [
    "t0 = DefaultTagger('NN')\n",
    "evaluate(t0, test)\n",
    "\n",
    "t1 = UnigramTagger(train, backoff=t0)\n",
    "evaluate(t1, test)\n",
    "\n",
    "t2 = BigramTagger(train, backoff=t1)\n",
    "evaluate(t2, test)\n",
    "\n",
    "t3 = nltk.tag.TrigramTagger(train, backoff=t2)\n",
    "evaluate(t3, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46958970845702974\n"
     ]
    }
   ],
   "source": [
    "trainer = nltk.tag.HiddenMarkovModelTrainer()\n",
    "hmm = trainer.train(train)\n",
    "evaluate(hmm , test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {\n",
    "        'last_letter': word[-2:].lower(),\n",
    "        'length': len(word),\n",
    "        'first_letter': word[:2].lower(),\n",
    "        'most_frequent_letter': Counter(word.lower()).most_common(1)[0][0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'il',\n",
       " 'length': 4,\n",
       " 'first_letter': 'nd',\n",
       " 'most_frequent_letter': 'n'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features('Ndil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names as names_corpus\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7944"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [(name, 'male') for name in names_corpus.words('male.txt')] + [(name, 'female') for name in names_corpus.words('female.txt')]\n",
    "random.shuffle(names)\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(gender_features(n), g) for (n, g) in names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "female\n",
      "0.766\n"
     ]
    }
   ],
   "source": [
    "print(clf.classify(gender_features('John')))\n",
    "print(clf.classify(gender_features('Mollie')))\n",
    "print(nltk.classify.accuracy(clf, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'na'           female : male   =    101.3 : 1.0\n",
      "             last_letter = 'la'           female : male   =     75.5 : 1.0\n",
      "             last_letter = 'ia'           female : male   =     39.9 : 1.0\n",
      "             last_letter = 'sa'           female : male   =     34.8 : 1.0\n",
      "             last_letter = 'rd'             male : female =     31.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "clf.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = names[1500:]\n",
    "devtest_names = names[500:1500]\n",
    "test_names = names[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tz.curry\n",
    "def extract_features(feature_function, dataset):\n",
    "    return [(feature_function(d), l) for (d, l) in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_gender_features = extract_features(gender_features)\n",
    "train_set = extract_gender_features(train_names)\n",
    "devtest_set = extract_gender_features(devtest_names)\n",
    "test_set = extract_gender_features(test_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816\n"
     ]
    }
   ],
   "source": [
    "clf = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(clf, devtest_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for (n, tag) in devtest_names:\n",
    "    guess = clf.classify(gender_features(n))\n",
    "    if guess != tag:\n",
    "        errors.append((tag, guess, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct=female guess=male name=Abbe\n",
      "correct=female guess=male name=Abbie\n",
      "correct=female guess=male name=Abby\n",
      "correct=female guess=male name=Bambie\n",
      "correct=female guess=male name=Beilul\n",
      "correct=female guess=male name=Buffy\n",
      "correct=female guess=male name=Bunnie\n",
      "correct=female guess=male name=Cam\n",
      "correct=female guess=male name=Chad\n",
      "correct=female guess=male name=Charlot\n",
      "correct=female guess=male name=Chloe\n",
      "correct=female guess=male name=Chrysler\n",
      "correct=female guess=male name=Clary\n",
      "correct=female guess=male name=Cyb\n",
      "correct=female guess=male name=Darby\n",
      "correct=female guess=male name=Darsey\n",
      "correct=female guess=male name=Debor\n",
      "correct=female guess=male name=Devi\n",
      "correct=female guess=male name=Devin\n",
      "correct=female guess=male name=Dorolice\n",
      "correct=female guess=male name=Dulce\n",
      "correct=female guess=male name=Edin\n",
      "correct=female guess=male name=Em\n",
      "correct=female guess=male name=Emmy\n",
      "correct=female guess=male name=Erin\n",
      "correct=female guess=male name=Esther\n",
      "correct=female guess=male name=Evvy\n",
      "correct=female guess=male name=Franky\n",
      "correct=female guess=male name=Gabey\n",
      "correct=female guess=male name=Gael\n",
      "correct=female guess=male name=Gennifer\n",
      "correct=female guess=male name=Gerry\n",
      "correct=female guess=male name=Gillian\n",
      "correct=female guess=male name=Goldy\n",
      "correct=female guess=male name=Grace\n",
      "correct=female guess=male name=Gray\n",
      "correct=female guess=male name=Grissel\n",
      "correct=female guess=male name=Haily\n",
      "correct=female guess=male name=Happy\n",
      "correct=female guess=male name=Harrie\n",
      "correct=female guess=male name=Hedwig\n",
      "correct=female guess=male name=Hephzibah\n",
      "correct=female guess=male name=Hilliary\n",
      "correct=female guess=male name=Honor\n",
      "correct=female guess=male name=Inez\n",
      "correct=female guess=male name=Jennifer\n",
      "correct=female guess=male name=Jewell\n",
      "correct=female guess=male name=Jo\n",
      "correct=female guess=male name=Joan\n",
      "correct=female guess=male name=Jocelin\n",
      "correct=female guess=male name=Jordan\n",
      "correct=female guess=male name=Jorry\n",
      "correct=female guess=male name=Joyan\n",
      "correct=female guess=male name=Joycelin\n",
      "correct=female guess=male name=Kipp\n",
      "correct=female guess=male name=Leland\n",
      "correct=female guess=male name=Margot\n",
      "correct=female guess=male name=Min\n",
      "correct=female guess=male name=Mommy\n",
      "correct=female guess=male name=Murial\n",
      "correct=female guess=male name=Muriel\n",
      "correct=female guess=male name=Nan\n",
      "correct=female guess=male name=Nitin\n",
      "correct=female guess=male name=Noel\n",
      "correct=female guess=male name=Noell\n",
      "correct=female guess=male name=Norean\n",
      "correct=female guess=male name=Paige\n",
      "correct=female guess=male name=Phylis\n",
      "correct=female guess=male name=Poppy\n",
      "correct=female guess=male name=Quinn\n",
      "correct=female guess=male name=Randy\n",
      "correct=female guess=male name=Renell\n",
      "correct=female guess=male name=Ricky\n",
      "correct=female guess=male name=Robby\n",
      "correct=female guess=male name=Rosalynd\n",
      "correct=female guess=male name=Shamit\n",
      "correct=female guess=male name=Sheelagh\n",
      "correct=female guess=male name=Shel\n",
      "correct=female guess=male name=Sibeal\n",
      "correct=female guess=male name=Sidoney\n",
      "correct=female guess=male name=Stace\n",
      "correct=female guess=male name=Theo\n",
      "correct=female guess=male name=Theresa-Marie\n",
      "correct=female guess=male name=Tiff\n",
      "correct=female guess=male name=Tiffy\n",
      "correct=female guess=male name=Tommy\n",
      "correct=female guess=male name=Tootsie\n",
      "correct=female guess=male name=Torrie\n",
      "correct=female guess=male name=Tuesday\n",
      "correct=female guess=male name=Vi\n",
      "correct=female guess=male name=Wallis\n",
      "correct=female guess=male name=Waly\n",
      "correct=female guess=male name=Wenonah\n",
      "correct=female guess=male name=Wileen\n",
      "correct=female guess=male name=Zoe\n",
      "correct=male guess=female name=Alasdair\n",
      "correct=male guess=female name=Andonis\n",
      "correct=male guess=female name=Andrej\n",
      "correct=male guess=female name=Andri\n",
      "correct=male guess=female name=Antonin\n",
      "correct=male guess=female name=Anurag\n",
      "correct=male guess=female name=Ari\n",
      "correct=male guess=female name=Augie\n",
      "correct=male guess=female name=Augustine\n",
      "correct=male guess=female name=Bengt\n",
      "correct=male guess=female name=Case\n",
      "correct=male guess=female name=Chaddie\n",
      "correct=male guess=female name=Chauncey\n",
      "correct=male guess=female name=Chris\n",
      "correct=male guess=female name=Chrissy\n",
      "correct=male guess=female name=Conway\n",
      "correct=male guess=female name=Corbin\n",
      "correct=male guess=female name=Dani\n",
      "correct=male guess=female name=Darryl\n",
      "correct=male guess=female name=Davie\n",
      "correct=male guess=female name=Derrol\n",
      "correct=male guess=female name=Dru\n",
      "correct=male guess=female name=Elisha\n",
      "correct=male guess=female name=Emmery\n",
      "correct=male guess=female name=Esteban\n",
      "correct=male guess=female name=Eustace\n",
      "correct=male guess=female name=Fidel\n",
      "correct=male guess=female name=Gabriele\n",
      "correct=male guess=female name=Garcia\n",
      "correct=male guess=female name=Germaine\n",
      "correct=male guess=female name=Giovanni\n",
      "correct=male guess=female name=Granville\n",
      "correct=male guess=female name=Gretchen\n",
      "correct=male guess=female name=Hannibal\n",
      "correct=male guess=female name=Henrique\n",
      "correct=male guess=female name=Herculie\n",
      "correct=male guess=female name=Herrmann\n",
      "correct=male guess=female name=Herve\n",
      "correct=male guess=female name=Hymie\n",
      "correct=male guess=female name=Ingamar\n",
      "correct=male guess=female name=Irwin\n",
      "correct=male guess=female name=Isa\n",
      "correct=male guess=female name=Jeffrey\n",
      "correct=male guess=female name=Jere\n",
      "correct=male guess=female name=Jeremie\n",
      "correct=male guess=female name=Jermaine\n",
      "correct=male guess=female name=Jonah\n",
      "correct=male guess=female name=Jose\n",
      "correct=male guess=female name=Joshuah\n",
      "correct=male guess=female name=Kaiser\n",
      "correct=male guess=female name=Karl\n",
      "correct=male guess=female name=Keene\n",
      "correct=male guess=female name=Konstantin\n",
      "correct=male guess=female name=Kyle\n",
      "correct=male guess=female name=Laurence\n",
      "correct=male guess=female name=Martainn\n",
      "correct=male guess=female name=Maury\n",
      "correct=male guess=female name=Meade\n",
      "correct=male guess=female name=Melvin\n",
      "correct=male guess=female name=Merle\n",
      "correct=male guess=female name=Merry\n",
      "correct=male guess=female name=Michael\n",
      "correct=male guess=female name=Micheil\n",
      "correct=male guess=female name=Montague\n",
      "correct=male guess=female name=Mugsy\n",
      "correct=male guess=female name=Noble\n",
      "correct=male guess=female name=Odell\n",
      "correct=male guess=female name=Orbadiah\n",
      "correct=male guess=female name=Peirce\n",
      "correct=male guess=female name=Percy\n",
      "correct=male guess=female name=Piggy\n",
      "correct=male guess=female name=Prasad\n",
      "correct=male guess=female name=Rabi\n",
      "correct=male guess=female name=Reube\n",
      "correct=male guess=female name=Rickie\n",
      "correct=male guess=female name=Rockwell\n",
      "correct=male guess=female name=Ronny\n",
      "correct=male guess=female name=Salvatore\n",
      "correct=male guess=female name=Sansone\n",
      "correct=male guess=female name=Serge\n",
      "correct=male guess=female name=Seymour\n",
      "correct=male guess=female name=Sonny\n",
      "correct=male guess=female name=Steven\n",
      "correct=male guess=female name=Tarrance\n",
      "correct=male guess=female name=Timothee\n",
      "correct=male guess=female name=Verne\n",
      "correct=male guess=female name=Vince\n",
      "correct=male guess=female name=Virge\n",
      "correct=male guess=female name=Wittie\n"
     ]
    }
   ],
   "source": [
    "for (tag, guess, name) in sorted(errors):\n",
    "    print(f'correct={tag} guess={guess} name={name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    (list(movie_reviews.words(fileid)), category)\n",
    "    for category in movie_reviews.categories()\n",
    "    for fileid in movie_reviews.fileids(category)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopword(word):\n",
    "    return word not in stopwords\n",
    "\n",
    "def process_words(words):\n",
    "    return tz.pipe(words, tzc.filter(filter_stopword), tzc.map(stemmer.stem))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(process_words(movie_reviews.words()))\n",
    "word_freqs =  Counter(all_words)\n",
    "# all_words = Counter(w.lower() for w in movie_reviews.words() if w not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = set(w for (w, f) in all_words.most_common(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = { f'contains({word})': True if word in present else False for word in sd }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, ngrams\n",
    "from nltk.metrics import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bigrams(words, scorer=BigramAssocMeasures.chi_sq, size=200):\n",
    "    print(words)\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "#     print(type(bigram_finder.score_ngrams(scorer)))\n",
    "    bigrams = bigram_finder.nbest(scorer, size)\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', ':', 'two', 'teen', 'coupl', 'go', 'church', 'parti', ',', 'drink']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tz.take(10, all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'coupl', 'go', 'church', 'parti', ',', 'drink']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(',', 'drink'),\n",
       " (':', 'two'),\n",
       " ('church', 'parti'),\n",
       " ('coupl', 'go'),\n",
       " ('go', 'church'),\n",
       " ('parti', ','),\n",
       " ('plot', ':'),\n",
       " ('teen', 'coupl'),\n",
       " ('two', 'teen')]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_bigrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(process_words(document))\n",
    "    contained = document_words.intersection(word_features)\n",
    "    features = { \n",
    "        f'contains({word})': True if word in contained else False \n",
    "        for word in document_words \n",
    "    }\n",
    "        \n",
    "    return features\n",
    "\n",
    "def document_features_bow(document):\n",
    "    document_words = set(process_words(document))\n",
    "#     contained = document_words.intersection(word_features)\n",
    "    features = { \n",
    "        f'contains({word})': True if word in document_words else False \n",
    "        for word in word_features \n",
    "    }\n",
    "        \n",
    "    return features\n",
    "\n",
    "def document_features_nltk(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "\n",
    "    for word in word_features:\n",
    "        features[f'contains({word})'] = word in document_words\n",
    "    return features\n",
    "\n",
    "extract_document_features = extract_features(document_features)\n",
    "extract_document_features_bow = extract_features(document_features_bow)\n",
    "extract_document_features_nltk = extract_features(document_features_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_features_bow(documents[-1][0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 ms  399 s per loop (mean  std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "extract_document_features(documents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.8 ms  1.53 ms per loop (mean  std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "extract_document_features_bow(documents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1 s  217 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "extract_document_features_nltk(documents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set\n",
      "devtest set\n",
      "test set\n",
      "CPU times: user 17.1 s, sys: 173 ms, total: 17.2 s\n",
      "Wall time: 17.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('train set')\n",
    "train_set = extract_document_features_bow(documents[200:])\n",
    "print('devtest set')\n",
    "devtest_set = extract_document_features_bow(documents[100:200])\n",
    "print('test set')\n",
    "test_set = extract_document_features_bow(documents[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71\n",
      "Most Informative Features\n",
      "      contains(outstand) = True              pos : neg    =     10.0 : 1.0\n",
      "        contains(seagal) = True              neg : pos    =      8.4 : 1.0\n",
      "         contains(mulan) = True              pos : neg    =      8.1 : 1.0\n",
      "        contains(poorli) = True              neg : pos    =      6.6 : 1.0\n",
      "          contains(lame) = True              neg : pos    =      6.5 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "clf = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(clf, devtest_set))\n",
    "print(clf.show_most_informative_features(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = []\n",
    "for (n, tag) in devtest_set:\n",
    "    guess = clf.classify(document_features(n))\n",
    "    if guess != tag:\n",
    "        errors.append((tag, guess, n))\n",
    "len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (tag, guess, name) in enumerate(sorted(errors, key=lambda d: d[:2])):\n",
    "    print(f'correct={tag} guess={guess} name={json.dumps(name, indent=2)}')\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_fdist = nltk.FreqDist()\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] += 1\n",
    "    suffix_fdist[word[-2:]] += 1\n",
    "    suffix_fdist[word[-3:]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of', 'the', 'y', 'r', 'to', 'in', 'f', 'o', 'ed', 'nd', 'is', 'on', 'l', 'g', 'and', 'ng', 'er', 'as', 'ing', 'h', 'at', 'es', 'or', 're', 'it', '``', 'an', \"''\", 'm', ';', 'i', 'ly', 'ion', 'en', 'al', '?', 'nt', 'be', 'hat', 'st', 'his', 'th', 'll', 'le', 'ce', 'by', 'ts', 'me', 've', \"'\", 'se', 'ut', 'was', 'for', 'ent', 'ch', 'k', 'w', 'ld', '`', 'rs', 'ted', 'ere', 'her', 'ne', 'ns', 'ith', 'ad', 'ry', ')', '(', 'te', '--', 'ay', 'ty', 'ot', 'p', 'nce', \"'s\", 'ter', 'om', 'ss', ':', 'we', 'are', 'c', 'ers', 'uld', 'had', 'so', 'ey']\n"
     ]
    }
   ],
   "source": [
    "print(list(tz.pluck(0, suffix_fdist.most_common(100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i):\n",
    "    features = {\n",
    "        'suffix(1)': sentence[i][-1:],\n",
    "        'suffix(2)': sentence[i][-2:],\n",
    "        'suffix(3)': sentence[i][-3:]\n",
    "    }\n",
    "    \n",
    "    if i == 0:\n",
    "        features['prev-word'] = '<START>'\n",
    "    else: \n",
    "        features['prev-word'] = sentence[i-1]\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suffix(1)': 'n', 'suffix(2)': 'on', 'suffix(3)': 'ion', 'prev-word': 'an'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_features(brown.sents()[0], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "featuresets = []\n",
    "for tagged_sent in tagged_sents:\n",
    "    untagged_sent = nltk.untag(tagged_sent)\n",
    "    for i, (word, tag) in enumerate(tagged_sent):\n",
    "        featuresets.append((pos_features(untagged_sent, i), tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'suffix(1)': 'e',\n",
       "  'suffix(2)': 'he',\n",
       "  'suffix(3)': 'The',\n",
       "  'prev-word': '<START>'},\n",
       " 'AT')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7891596220785678"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "clf = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(clf, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_feature_history(sentence, i, history):\n",
    "    features = {\n",
    "        'suffix(1)': sentence[i][-1:],\n",
    "        'suffix(2)': sentence[i][-2:],\n",
    "        'suffix(3)': sentence[i][-3:]\n",
    "    }\n",
    "    \n",
    "    if i == 0:\n",
    "        features['prev-word'] = '<START>'\n",
    "        features['prev-tag'] = '<START>'\n",
    "    else:\n",
    "        features['prev-word'] = sentence[i-1]\n",
    "        features['prev-tag'] = history[i-1]   \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in tagged_sents:\n",
    "            untagged_sent = nltk.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (_, tag) in enumerate(tagged_sent):\n",
    "                featuresets = pos_feature_history(untagged_sent, i, history)\n",
    "                history.append(tag)\n",
    "                train_set.append((featuresets, tag))\n",
    "        self.clf = nltk.NaiveBayesClassifier.train(train_set)\n",
    "        \n",
    "    def tag(self, sentence):\n",
    "        tagged_sent = []\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featuresets = pos_feature_history(sentence, i, history)\n",
    "            tag = self.clf.classify(featuresets)\n",
    "            history.append(tag)\n",
    "            tagged_sent.append((word, tag))\n",
    "        return tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n",
    "tagger = ConsecutivePosTagger(train_sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NN'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'CC'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN'),\n",
       " (\"Atlanta's\", 'NP$'),\n",
       " ('recent', 'JJ'),\n",
       " ('primary', 'NN'),\n",
       " ('election', 'NN'),\n",
       " ('produced', 'VBD'),\n",
       " ('``', '``'),\n",
       " ('no', 'AT'),\n",
       " ('evidence', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('that', 'CS'),\n",
       " ('any', 'DTI'),\n",
       " ('irregularities', 'NNS'),\n",
       " ('took', 'VBD'),\n",
       " ('place', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(nltk.untag(test_sents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN'),\n",
       " (\"Atlanta's\", 'NP$'),\n",
       " ('recent', 'JJ'),\n",
       " ('primary', 'NN'),\n",
       " ('election', 'NN'),\n",
       " ('produced', 'VBD'),\n",
       " ('``', '``'),\n",
       " ('no', 'AT'),\n",
       " ('evidence', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('that', 'CS'),\n",
       " ('any', 'DTI'),\n",
       " ('irregularities', 'NNS'),\n",
       " ('took', 'VBD'),\n",
       " ('place', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction, preprocessing, pipeline\n",
    "from sklearn import naive_bayes, linear_model, tree\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tagged_dataset(tagged_sents):\n",
    "    X, y = [], []\n",
    "    for tagged_sent in tagged_sents:\n",
    "        untagged_sent = nltk.untag(tagged_sent)\n",
    "        history = []\n",
    "        for i, (_, tag) in enumerate(tagged_sent):\n",
    "            featuresets = pos_feature_history(untagged_sent, i, history)\n",
    "            history.append(tag)\n",
    "            X.append(featuresets)\n",
    "            y.append(tag)\n",
    "    return X, y\n",
    "\n",
    "def predict_tags(estimator, labelizer, sentence):\n",
    "    history = []\n",
    "    for i, word in enumerate(sentence):\n",
    "        features = pos_feature_history(sentence, i, history)\n",
    "        label = estimator.predict(features)\n",
    "        tag = labelizer.inverse_transform(label)\n",
    "        history.append(tag[0])\n",
    "    return list(zip(sentence, history))\n",
    "        \n",
    "        \n",
    "class ConsecutivePosTagEstimator(nltk.TaggerI):\n",
    "    def __init__(self, clf, train_sents):\n",
    "        self.estimator = pipeline.make_pipeline(feature_extraction.DictVectorizer(), clf)\n",
    "        features, tags = create_tagged_dataset(train_sents)\n",
    "        self.labelizer = preprocessing.LabelEncoder()\n",
    "        y = self.labelizer.fit_transform(tags)\n",
    "        self.estimator.fit(features, y)\n",
    "        \n",
    "    def tag(self, sentence):\n",
    "        return predict_tags(self.estimator, self.labelizer, sentence)\n",
    "    \n",
    "    def tag_sents(self, sentences):\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            tagged_sents = executor.map(self.tag, sentences)\n",
    "        return tagged_sents\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_p, devtest_p = 0.1, 0.1\n",
    "test_i, devtest_i = int(len(tagged_sents) * test_p), int(len(tagged_sents) * (test_p + devtest_p))\n",
    "test_sents, devtest_sents, train_sents = tagged_sents[:test_i], tagged_sents[test_i:devtest_i], tagged_sents[devtest_i:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, tags_train = create_tagged_dataset(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'suffix(1)': 'e',\n",
       "  'suffix(2)': 'He',\n",
       "  'suffix(3)': 'He',\n",
       "  'prev-word': '<START>',\n",
       "  'prev-tag': '<START>'},\n",
       " {'suffix(1)': 'd',\n",
       "  'suffix(2)': 'ed',\n",
       "  'suffix(3)': 'ted',\n",
       "  'prev-word': 'He',\n",
       "  'prev-tag': 'PPS'},\n",
       " {'suffix(1)': 't',\n",
       "  'suffix(2)': 'st',\n",
       "  'suffix(3)': 'ast',\n",
       "  'prev-word': 'cited',\n",
       "  'prev-tag': 'VBD'}]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelizer = preprocessing.LabelEncoder()\n",
    "y_train = labelizer.fit_transform(tags_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = preprocessing.OneHotEncoder()\n",
    "# X_train = encoder.fit_transform(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = naive_bayes.BernoulliNB()\n",
    "# clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('dictvectorizer', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=True)), ('bernoullinb', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = pipeline.make_pipeline(feature_extraction.DictVectorizer(), clf)\n",
    "estimator.fit(features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BEDZ-HL'], dtype='<U11')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelizer.inverse_transform([30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'AT'),\n",
       " ('assured', 'VBD'),\n",
       " ('Mr.', 'NP'),\n",
       " ('Martinelli', 'NP'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'AT'),\n",
       " ('council', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('he', 'AT'),\n",
       " ('would', 'NN'),\n",
       " ('study', 'NN'),\n",
       " ('the', 'AT'),\n",
       " ('correct', 'NN'),\n",
       " ('method', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('report', 'NN'),\n",
       " ('back', 'NN'),\n",
       " ('to', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('council', 'NN'),\n",
       " ('as', 'NNS'),\n",
       " ('soon', 'NN'),\n",
       " ('as', 'CS'),\n",
       " ('possible', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_tags(estimator, labelizer, nltk.untag(devtest_sents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.01 s, sys: 152 ms, total: 1.16 s\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "naive_bayes_tagger = ConsecutivePosTagEstimator(naive_bayes.BernoulliNB(), train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.5 s, sys: 19.9 s, total: 38.4 s\n",
      "Wall time: 5min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.601364623177918"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "naive_bayes_tagger.evaluate(devtest_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
